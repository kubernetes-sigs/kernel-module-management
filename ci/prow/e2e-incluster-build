#!/usr/bin/env bash

set -euxo pipefail

echo "Deploy KMMO..."
make deploy

echo "Check that the kmm_ci_a module is not loaded on the node..."
if minikube ssh -- lsmod | grep kmm_ci_a; then
 echo "Unexpected lsmod output - the module is present on the node before the module was applied to the cluster"
 exit 1
fi

echo "Create a build secret..."
oc create secret generic build-secret --from-literal=ci-build-secret=super-secret-value

echo "Add a configmap that contain the kernel module build dockerfile..."
kubectl apply -f ci/kmm-kmod-dockerfile.yaml

echo "Add an kmm-ci Module that contains a valid mapping..."
kubectl apply -f ci/module-kmm-ci-build.yaml

# Wait for the build pod to be created. `kubectl wait` doesn't support such option,
# see https://github.com/kubernetes/kubernetes/issues/83242.
echo "Waiting for the build pod to be created..."
timeout 1m bash -c 'until kubectl get pods -o json | jq -er ".items[].metadata.name | select(.? | match(\"build\"))"; do sleep 1; done'
POD_NAME=$(kubectl get pods -o json | jq -r '.items[].metadata.name | select(.? | match("build"))')

# we can't exec a command nor get the logs on a pod that isn't `Running` yet.
kubectl wait pod/${POD_NAME} --for jsonpath='{.status.phase}'=Running --timeout=60s

# The build job/pod is deleted once done so we won't be able to get this info later on in the troubleshooting section.
echo "Print the build logs..."
kubectl logs pod/${POD_NAME} -f

echo "Check that the module gets loaded on the node..."
timeout 10m bash -c 'until minikube ssh -- lsmod | grep kmm_ci_a; do sleep 3; done'

echo "Remove the Module..."
kubectl delete -f ci/module-kmm-ci-build.yaml

echo "Check that the module gets unloaded from the node..."
timeout 1m bash -c 'until ! minikube ssh -- lsmod | grep kmm_ci_a; do sleep 3; done'

