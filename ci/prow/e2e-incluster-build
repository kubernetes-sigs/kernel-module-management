#!/usr/bin/env bash

set -euxo pipefail

echo "Deploy KMMO..."
make deploy

echo "Create a build secret..."
kubectl create secret generic build-secret --from-literal=ci-build-secret=super-secret-value

echo "Add a configmap that contain the kernel module build dockerfile..."
kubectl apply -f ci/kmm-kmod-dockerfile.yaml

echo "Add an kmm-ci Module that contains a valid mapping..."
sed -e "s/KVER_CHANGEME/$(minikube ssh -- uname -r)/g" ci/module-kmm-ci-build.template.yaml | tee module-kmm-ci.yaml
kubectl apply -f module-kmm-ci.yaml

# Wait for the build pod to be created. `kubectl wait` doesn't support such option,
# see https://github.com/kubernetes/kubernetes/issues/83242.
echo "Waiting for the build pod to be created..."
timeout 1m bash -c 'until kubectl get pods -o json | jq -er ".items[].metadata.name | select(.? | match(\"build\"))"; do sleep 1; done'
POD_NAME=$(kubectl get pods -o json | jq -r '.items[].metadata.name | select(.? | match("build"))')

# The build job/pod is deleted once done so we won't be able to get this info later on in the troubleshooting section.
echo "Print the build logs..."
# we can't get the logs on a pod that isn't `Running` yet.
kubectl wait pod/${POD_NAME} --for jsonpath='{.status.phase}'=Running --timeout=60s
kubectl logs pod/${POD_NAME} -f

echo "Check that the module gets loaded on the node..."
timeout 10m bash -c 'until minikube ssh -- lsmod | grep kmm_ci_a; do sleep 3; done'


echo "Remove the Module..."
kubectl delete -f module-kmm-ci.yaml

echo "Check that the module gets unloaded from the node..."
timeout 1m bash -c 'until ! minikube ssh -- lsmod | grep kmm_ci_a; do sleep 3; done'

